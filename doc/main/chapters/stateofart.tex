\chapter{State of the Art review}

On previous approaches that shares similar motivations with this thesis and a
discussion about them. 

\section{Previous and related work}\label{related_work}

The analyzed literature can be split into two main subsets. By one hand we have 
the syntactic structure analysis tools that provide information about the actual 
program structure and by the other hand the behavioral structure analysis tools
that want to expose the different phases on an execution in terms of performance. 
The former subset of works is the most related to our goals even if not all of
them are using the structure detection for improve analysis but also for
trace compression. The last subset is not so related but it have been considered
interesting to analyze them since provide valuable insights for our purposes.

\subsection{Syntactic structure}\label{ss:syntactic_structure}

This section has been divided into two subsections. The first is about
algorithms used on an on-line structure analysis, i.e. Overall trace is not 
available from the beginning so the structure is being detected while the
application is being executed. The works presented in this sections are 
mainly concerned about reduce the size of the trace by compressing these
repetitive parts so they are facing the tracing scalability problem. The second 
section is about off-line structure analysis, that unlike before trace is 
available so a trace post-mortem analysis is done.
 
\subsubsection{On-line structure analysis}

In \cite{noeth2009scalatrace} they are concerning about the scalability of the
tracing part. They claim reductions of about a
thousand in terms of trace size just by detecting the structure of the
application, e.g. If the same thing is repeated 100 times, just saving it once
and tagging with the number of times it is executed should be enough. They
propose to use RSD (Regular Section Descriptors) to express MPI events nested
inside a loop and a sequence analysis algorithm for detect the repetitive 
patterns. Their compression
is done in two phases. The first one is an intra-node compression, where the
repetitive patterns arise and the second one is inter-node merge, where all
single-node compressions are merged forming the whole application trace.
They maintain two sequences the ``target'' that contains the already detected
sequences sets and the ``match'' sequence that is formed by the newly acquired
trace records. The compression algorithm maintains a queue of MPI events and 
attempts to greedily compress the first matching by  in four steps procedure:
\begin{enumerate}[label=\roman*)]
  \item Head and tail of the match sequence is determined by traversing the
    queue backwards such that the last item is the tail named ``target tail''
    and the next coincident item with this tail is the ``match tail'' so just
    the previous one will be the ``target head''.
  \item Following from ``match tail'' find out the item that is equal as
    ``target head''. This will be the ``match head''.
  \item An element-wise comparisson is performed in order to check out whether 
    both sequences match or not.
  \item If there is a match the ``match'' sequence is merged to the ``target''
    and is removed from the queue.
\end{enumerate}
Also they claim
that even if their main target is to compress traces, Scalatrace also can be
used for analyze the application structure by doing a little demo showing how it
can detect the most outer loop iterations or timesteps. One of the main
drawbacks of this approach is that the complexity of intra-node compression 
can be of $O(n^2)$ nevertheless they are avoiding this by
limiting the algorithm search (first step) with a windows size. They claim that
with a windows of 500 events is used to be enough. 

In \cite{aguilar2014mpi} they present the Event Flow Graphs (EFG). EFG are
weighted directed graphs where every node is an MPI call and edges the
transitions between them being the weight the number of transitions done
from one node to the other, so the program code blocks executed between them.
Graph nodes can contain aggregate information like call duration or message size
and edges can be attached with information about CPU burst like performance
metrics like IPC. 
The EFG is constructed like in Scalatrace at monitoring time and it consists on
two basic actions: 
\begin{enumerate*}[label=\roman*)]
  \item Every time an MPI call is detected gather all information and store it
    in a hashmap indexed by the MPI call signature. The signature is a k-tuple
    of components which represents relevant metrics like MPI call type and
    source code position. Every entry of the hashmap is directly related with
    one node.
  \item Also on every MPI call detection a transition from one node to other has
    to be stored, it is what they called ``signature history'' and it consists
    on a set of pairs $(signature_{i-1}, signature_{i})$ and for every pair an
    scalar value is also stored that indicates number of times this transition
    is taken. This set of transitions are the edges of the EFG.
\end{enumerate*}

So far no information about order is taken into account
so additionally they present in \cite{aguilar2016event} temporal-EFG that 
introduce more
information for these cases where the execution order can not be reconstructed
with the previous EFG. They claim this technique can be used for trace
compression, application structure detection and visual performance analysis.
Following with application structure detection, what is where this thesis is
focused on, they use algorithms for cycle detection over the t-EFG (DFS-based)
and once cycles are detected the graph is transformed to a hierarchical tree
where loops and subloops are showed up. Statistics about loops can be gathered
like number of iterations, total time in loops and so on.

\subsubsection{Offline structure analysis}

Some of works in this section are out of the HPC field,
this is because the structure extraction is also useful for other purposes like
reverse engineering on interactive software nevertheless they are actually useful
since the objective is quite the same.

Starting with \cite{Safyallah2006} they propose analyze execution
traces of software systems in order to extract the intern structure for
improving reverse engineering process. For
that end they first instrument the application (the entry/exit of functions) and 
a set of relevant task scenarios (actions) are selected, that examine a 
single software feature, called feature-specific scenario set. These scenarios
are executed and traces are collected. Second step is the execution pattern
analysis that extract both, intra-scenario set where patterns that are specific
to a single software feature and inter-scenario set where more general
patterns appears, i.e. no feature specific structure so the main structure of 
the application. For pattern analysis they rely on sequential pattern mining
techniques using a modified version from \cite{Agrawal_seqpatt} what is a
slightly different from what has been explained on \ref{ss:temporal_sequences} 
and is also using candidates generation approach. They are able to find inter
and intra-scenario patterns by tuning the Minimum support such that for detect
feature-specific patterns it is reduced to about 5\% and to find out
inter-scenario patterns it is incremented to about 25\%.

Similarly in \cite{Zhao2008} they present an approach to extract the intern
structure of software systems, mainly interactive, but using graph-based 
substructure mining algorithms instead of sequential based mining techniques.
They propose a four step methodology: 
\begin{enumerate*}[label=\roman*)]
  \item Trace collection
  \item Trace preprocessing
  \item Grammar induction
  \item and Grammar parsing.
\end{enumerate*}
Traces have several information but the most important is the enter and exit
from methods that allows to derive the call-graphs. These call-graphs are saved
as a linked list of caller-callee relations. Following, to facilitate the
grammar induction step, the data is simplified by remove some repetitive and
fine-grain details such that low-level methods. Note
the difference in this point, we rely on the analysis of repetitive patterns as
a source for infer the internal structure while they identify loops repetitions
as a phenomena that do not contribute to the structural feature. It can be
explained taking into account the different targeted kind of software. The
result of trace preprocessing feeds the grammar induction step that rely on
VEGGIE (Visual Environment for Graph Grammars: Induction and Engineering). 
Induction algorithm iteratively finds common substructures from a
given set of data, and organizes the hidden hierarchical substructures in a
gramatical way. When a common frequent substructure is found, a grammar
production will be created.

This paper considers to use a different approach from sequential pattern mining
and it is also interesting since the last seems is the dominant approach. They show up
some results being maybe acceptable for the targeted analysis but not for this
thesis objectives since they report execution times of about 70 seconds for
traces with about 90 events.

% In \cite{Zou2010} \ldots

In a bit different scenario, \cite{Lopez-Cueva2012} they talk about debugging 
and optimization process of
software for SoCs\footnote{System on Chip} by means of traces post-mortem
analysis. They explain the complexity of SoCs drives the analysis of these
traces difficult because the high quantity of information that they are used to
collect so they are facing the problem about scalability on analysis. They argue
that the manual analysis of execution traces is becoming an unmanageable task so
this task has to be aided by automatically extract pertinent information, what
is in fact the structure of the application, and for
that they rely on pattern mining techniques, specifically frequent periodic
pattern mining. As has been explained in section \ref{pattern_mining} this sort
of algorithms are used to work with set of transactions so in order to adapt
trace mining to that algorithms they have chosen to split the trace into a set
of subtraces (by time intervals or by function name). They say \textit{``we are
interested in discovering sets of events that occur periodically [\ldots] but
the order is not taken into account [\ldots] the order can change according to
the scheduler (in a multi-thread environment)''}. It remembers to the algorithm
explained at section \ref{ss:temporal_sequences} in the sense that the events in
windows were not assessed to be in order since they could happen in parallel.
Furthermore the split of the trace into transactions remembers to the windows
explained in that same section. Additionally to the classical temporal sequence
mining they introduce the definition of cycles as \textit{``When an itemset  occurs over 
a set of transactions and the distance between any two consecutive transactions 
is constant''} and periodic pattern as \textit{``a set of consecutive cycles over the
same itemset and the same period''}. One of the interesting concepts that is a
bit out of the scope of this thesis is that another of the goals of this paper
is to recognize the gaps, i.e. the disruptions into the periodicity of a
pattern. They have these information because trace information is not just from
application point of view but from a whole SoC point of view. 

The mining consists on a four step algorithm where the first one is the
responsible of find out all the cycles of all possible periods (from 1 to \#transactions / min. support) containing a selected item. The rest of steps are
responsible to refine the output in order to end up with the minimum useful
information. First step is intuitively slow and generates a highly redundant
information so is hardly scalable.


Returning to the HPC field, in \cite{trahay2015selecting} they present an 
approach for select points of
interest automatically from an execution trace, understanding as points of 
interest these iterations that
behave different from the majority. The first phase is a post-mortem analysis of
a given trace. This analysis is about finding patterns of events that are
repeated, i.e. folding loops that has been unfolded during the
tracing\footnote{Unfolding in the sense that tracing process represent loops as
a sequence of a repeated set of events.}. Their algorithm is about finding short
repeated sequences of events and try to expand the pattern. After detect the
intern structure of an application, the analysis of durations of the
different iterations of the detected loops is an arbitrary construction, once
done, they filter all iterations that behave similar and
expose to analyst these iterations that are outliers assuming these are the
most interesting ones.

They propose a iterative three steps algorithm:
\begin{enumerate}[label=\roman*)]
  \item Find out a sequence of two consecutive events that appears several
    times. All the subsequences are then replaced with a pattern construct
    $p_{1}$.
  \item Next step is about finding loops composed of $p_{1}$. This is done by
    comparing every $p_{1}$ with next event, if they are equal then both are
    grouped into a loop.
  \item Last step is about expanding the pattern $p_{1}$ by looking at following
    event. If all $p_{1}$ have the same following event then it is integrated,
    if several of them shares the same following event, new pattern $p_{2}$ is
    created otherwise the pattern can not be expanded.
\end{enumerate}
Steps 2 and 3 are repeated until no more expansions can be done. Then the
process starts again from step 1 until no more pairs can be found.

This algorithms can be classified as a pattern growing algorithm (described in
section \ref{ss:projection_based}) but without the projection step so all data
is traversed again and again. They said their algorithm is dominated by the
first step that presents a complexity of $O(n^2)$ with the length of the
patterns.

Compressed Complete Call Graphs (cCCG) was
presented by \cite{knupfer2005construction}. It can be said it is about 
profilizing a trace. It consist on finding repetitive 
patterns for loosely or lossless compression.  
CCG is basically a graph of function calls of a program so the main structure is
defined by the function call hierarchy while additional information are appended
usually as leaf nodes. The construction phase is quite simple. The trace is
traversed in a sequential manner, every time a function enter event is detected,
new node is generated and append to the current active node. The
other way around when exit function event is read, the current active node is
finalized and all information concerning to this node is presented e.g.
duration. Additionally, while constructing, the graph is compressed. The basic 
idea is to replace $n$ repeated sub-trees that are equal or similar with a 
reference to a single instance saving $n-1$ remaining copies. For similar is
understood that when comparing nodes not all properties must be equal for
example is not needed some scalar values like duration match perfectly (some
configured deviation is acceptable) but properties like function id must. They
claim compression ratio about 200 can be achieved with this approach.
This approach improves profiling in the sense that same function with same call
path and different time behaviour is exposed to analyst but still presents a lack 
of information on the order of the execution of the different graph paths so
application structure is exposed just partially. The cost of this algorithm is 
lineal since no back search is needed. It is
because the entry and exit function information is actually on the trace. If it
were not, the only manner to be sure if the next event belongs to the same
iteration or to the next is by means of a back search similarly to the presented
in \cite{noeth2009scalatrace} and as have been argued before it can lead to
quadratic costs. The fact of having the entry/exit information in traces adds a
non negligible overhead what implies the poor scalability of this
method in terms of tracing.

%% TODO: Talk about Cube???

\subsection{Behavioral structure}\label{ss:behavioral_structure}

Consider the same bunch of code will behave in the same
manner in general, it could be assumed that there is a powerful correlation 
between the behavior and the code structures. From which can be said a 
behavioral analysis is a side-channel analysis because indirectly the internal 
syntactic structure can be
betrayed. This consideration is incorrect in general so the
main goal of the different approaches presented in this section is not
to present a syntactic but a behavioral structure. The motivation is that when
analyst deals with syntactic structure, like in profilers, time variations of 
the same functions is
hidden, this situation can appears for example when calling same function with
different parameters. For the analyst point of view could be more interesting to
have this information unfolded. Take into account that this same property can
end up identifying different parts of the code as the same phase.

Even if the goal of the approaches explained below does not match with the
goal of this thesis, they provide a really useful insights as a related work.

In \cite{casas2007automatic} they propose 
automatically extract the internal structure of an MPI application from a
Paraver tracefile and provide to analyst just representative phases and they 
rely on signal analysis for this propose. 
Their analysis consists on two main steps:
\begin{enumerate*}[label=\roman*)]
    \item The first is to clean-up the trace by identify the perturbed regions.
Perturbed regions are those parts of the trace that has been perturbed nor by
the application nor by architecture but by external factors such that unknown
system activity or tracing package. Their clean-up phase is centered on remove
noise from tracing package, i.e. flush to disk. By building up a signal based
on flushing events and transform it by Closing morphological filter they end up
with this perturbed phases. 
    \item The second step is the identification of the
phases. It is done by means of autocorrelation and periodicity analysis of a
signal. This signal is build up from any metric like instantaneous FLOPs but
they use number of MPI point to point calls being executed. Once the period is
successfully detected the same process is done recursively on one of the
periods. This allows to have a hierarchical structure. 
\end{enumerate*}
Finally the output is
basically information about the different phases like number of iteration and
timing plus some representative cuts of the original trace. 

In \cite{gonzalez2013application} propose a
technique to identify the different execution phases using clustering 
techniques. Different parts of the
execution are considered as the same phase depending on the behavior,
so an analysis using some hardware metrics is
performed. The units to be cluster are segments of the execution to be defined,
i.e. execution bursts that takes place between a given events such as MPI
functions or events fired by the user to trace. The first step is about reducing 
the complexity of the clustering by filtering the less relevant of these 
computation burst, i.e. little bursts according with a given threshold. The 
second step is the clustering itself. These defined execution burst will have
behavioral information attached such as time or hardware counters that will be
used for cluster them in a user defined N-dimensional space. In order to 
reduce the dimensionality the typical approach is to
use two different dimension sets: 
\begin{enumerate*}[label=\roman*)]
  \item  Completed Instructions against IPC. This configuration provides a
        performance view. 
  \item  The second is Completed instructions against L1 and L2 cache
        misses. This combination reflects the impact of the architecture on the
        application structure. 
\end{enumerate*}
Once the clustering is done, this information
can be sent back to the trace and can be visualized by the analyst. Additionally
an interesting analysis can be done with the shape of clusters like for example,
working with IPC vs. Number of instructions dimensions if the shape of a given 
cluster is
flat on the second axis it means that there is an imbalance on instructions.
Clustering in the field of performance analysis was used before this proposal 
just for classify processes that behaves similar so its utilization by
identifying different phases on temporal dimension opened the door to new research
paths.

\section{Discussion}\label{s:soa_discussion}

The syntactic structure proposals (in \ref{ss:syntactic_structure}) have been 
divided into two subsections because has been considered they belongs to a 
different scenario and so different approaches were proposed. The
first is about those algorithms that works online with the execution of the
target software. These algorithms are not dealing with the entire trace but they
are constantly feed by the instrumentation package that is currently monitoring 
the application. They are discovering the patterns by means of a sort of
aggregative mechanism. On the first paper explained they are aggregating
incoming sets of events with current detected patterns, the second papers they
are aggregating the events and also the transitions. Additionally on
\cite{noeth2009scalatrace} they introduce useful concepts as:
\begin{enumerate}[label=\roman*)]
  \item Calling sequence identification used for unambiguously identify
    different MPI calls that lie on different code positions. It is an important
    matter because if we do not take into account we would end up having aliasing
    problems, different calls would seems the same. 
  \item Recursion folding signatures for dealing with recursion. If an MPI call 
    is called recursively, the signature based on the callstack would identify
    same call as different. The proposal is to fold the same call on the call
    stack, e.g. $A\rightarrow A \rightarrow A \rightarrow MPI$ will be $A
    \rightarrow MPI$.
\end{enumerate}
A clear limitation of this approach is the use of the windows workaround
that limits the recognition of large patterns. In \cite{aguilar2016event}
they propose to use graphs for application structure inspection. After collect
and build up the tEFG, they apply a DFS-analysis in order to give to the user
a hierarchical graph where the structure is revealed but what they no say (and
it seems to be because the figure they show) is that this derived graph loss
temporal information, so it can be considered a limitation of the methodology
(also we have to say that this is not the main goal of the proposal). From both
papers there is a valuable information (most in the second one) about the
idiosyncrasy of HPC applications in what this thesis rely.
\cite{noeth2009scalatrace} say \textit{``[\ldots] the outermost loop of the code
that contained repeated MPI calls. This timestep loop is of particular interest
for performance modeling as convergence algorithms are often based on either
fixed iteration bounds for the number of timesteps or epsilon-based error 
constraints resulting in input-specific number of timesteps. [\ldots]''} and in \cite{aguilar2016event} they talk
about the ``big outer loop hypothesis''. Additionally says \textit{``[\ldots] by
detecting the graph cycles (nodes are MPI calls), we are detecting the actual
loops that drive the simulation in the application''}. What it means is that
monitoring the MPI library is enough for having a clue about the general 
structure of the application, they acts as the fundamental pillars.

In off-line syntactic structure algorithms, that
is in fact the approaches what shares more things with our objectives. The
dominant algorithms here are the sequential pattern mining algorithms with
variations for every case. Because the importance of these algorithms for this
field, a previous study of them have been driven. If you want more details go to
\ref{pattern_mining}. Even if the structure of the applications can be
betrayed successfully, the sequential pattern mining, particularly for candidate 
generation-and-test techniques, and when low support is used, performance can 
degrade dramatically. In apriori algorithm presented in \ref{ss:apriori_based} 
have been observed that
the number of candidates needs to be generated for a relatively long itemsets is
really huge and presents an exponential growth with the number of items in the
itemset. Additionally for every one of these candidates, data needs to be
traversed again on the test step.

If we look closer, one can realize that the complexity of the algorithm mostly 
depends on the expectations of the complexity of the betrayed patterns. 
About studied literature in syntactic 
structure \cite{Safyallah2006} \cite{Zhao2008} talks about get the 
application structure at level of entry/exit functions, and as they talk it can 
leads to a high complex structures that needs post-process, for them 
pattern mining algorithms is the natural choice. In \cite{trahay2015selecting}
they talk about find patterns of events that are both, MPI calls and function
calls in general and following the former, rely on pattern mining. 
\cite{noeth2009scalatrace} also use some sort of sequential pattern mining but 
the feeling is that their needs drivers to a more simple algorithm. Finally in
\cite{aguilar2016event} they just perform a directed graph building because they
just get MPI calls information, so simpler patterns will be detected. Strong
reason for prefer a simplified analysis for the last two is they are working 
online, so they are trying to decrease the overhead as much as possible.

About last set of proposals, those classified as behavioral
structure approaches (in \ref{ss:behavioral_structure}), it can be seen signal
processing and general data mining techniques are used. The reason is
because the behavior is expressed with scalar values like number of
instructions, cycles, cache misses, etc instead of by a sequence of a 
defined set of events like MPI calls so sequences pattern mining is not the best
choice. About \cite{casas2007automatic} it shares with this thesis the motivation 
explained in \ref{ss:mot_regions_of_interest}. They can present a
hierarchical graph with detected structure but without temporal order
information what is a weak spot. About \cite{gonzalez2013application} it brings 
the idea of the use
of clustering for structure detection, using burst clustering with hardware 
metrics you can have aliasing in the sense that two burst that execute different
parts of the code can be behave similar and lie on same cluster but with other
metrics, actual code structure can be betrayed as is demonstrated in this
thesis.
